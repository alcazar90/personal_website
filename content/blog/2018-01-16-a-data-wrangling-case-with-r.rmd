---
title: A data wrangling case with spreadsheets using R
author: Cristóbal Alcázar
date: '2018-01-16'
slug: a-data-wrangling-case-with-r
categories: [R, TidyVerse]
tags: [R, TidyVerse]
comments: yes
showcomments: yes
showpagemeta: yes
---
```{r set-options, echo = FALSE, cache = FALSE}
options(width = 50)
```

```{r, message=FALSE, echo=FALSE}
library(dplyr)
```

![](img/spreadsheet_shock-min.jpg#center){ width=60% }


A typical real world situation is the problem of collecting data from a set of files and structured into one dataset. *"Why data is sparse?"* you can think, well this could happen for multiple reasons. For example, some phenomenon are measured along time units and you can only access their data by a period, others are a collection of business units and the origin's data was not centralized. So you end with one file per (time or business) unit in your hands.

Another typical situation is that data is shared or is avaiable in excel files. That is because excel is the main used analytical tool in the world (I guess!) and many of us didn't have a class of best practices on how to share and document data. The problem with excel files are the "spreadsheet intrinsic issues". What is that?...is just a fancy way to call the messy format problems that are common to find in spreadsheet files like multiple header levels, more than one table per sheet, different data types values in same column, and the list goes on.

You can look by yourself what I am trying to say in the following magistral example, but first some context. Below of this paragraph you can find a series of tweets that belong to very well known data scientists. The discussion is about a challenge to turn <a href="https://t.co/v9ucC0Vj2t" target="_blank">this spreadsheet</a> (a xlsx file) into a tidy data and <a href="http://varianceexplained.org/" target="_blank">David Robinson (*the challenged*) </a>deal with the problem in  <a href="http://rpubs.com/dgrtwo/tidying-enron" target="_blank">a very simple and fluently way</a>.

`r blogdown::shortcode('tweet', '717815339480977410#center')`

`r blogdown::shortcode('tweet', '718170667573579776')`

The rest of the post is dedicated to highlight some steps of David Robinson's answer to the tweet challenge, and then apply the insights on a case of data contained in multiple "xlsx" files with many sheets.

### Step 1: The coordinate-view

Consider the next innocent table as an example.

![](img/murdervictims.png#center){ width=65% }

It's easy to import the above spreadsheet with the `readxl::read_excel` function and obtain a rectangular table in R, filled with missing values (NA) instead of blank cells. You can take a look of the first six rows.

```{r, eval=FALSE}
library(readxl)
df <- readxl::read_excel("./innocent_table.xlsx")
df
```

```{r, echo=FALSE}
colA <- c("Expanded Homicide Data Table 8", "Murder Victims", "by Weapon, 2004-2008", "Weapons", "Total", "Total firearms:", "Handguns", "Rifles", "Shotguns", "Other guns", "Firearms, type not stated", "Knives or cutting instruments", "Blunt objetcs (clubs, hammers, etc)", "Personal weapons (hands, fist, feet, etc)1", "Poison", "Explosives", "Fire", "Narcotics", "Drowning", "Strangulation", "Asphyxiation", "Other weapons or weapons not stated", "1 Pushed is included in personal weapons")
colB <- c(NA, NA, NA, 2004, 14210, 9385, 7286, 403, 507, 117, 1072, 1866, 667, 
          943, 13, 1, 118, 80, 16, 156, 109, 856, NA)
colC <- c(NA, NA, NA, 2005, 14965, 10158, 7565, 445, 522, 138, 1488, 1920, 608,
          905, 9, 2,  125, 46, 20, 118, 96, 958, NA)
colD <- c(NA, NA, NA, 2006, 15087, 10225, 7836, 438, 490, 107, 1354, 1830, 618, 
          841, 12, 1, 117, 48, 12, 137, 106, 1140, NA)
colE <- c(NA, NA, NA, 2007, 14916, 10129, 7398, 453, 457, 116, 1705, 1817, 647, 869, 
          10, 1, 131, 52, 12, 134, 109, 1005, NA)
colF <- c(NA, NA, NA, 2008, 14180, 9484, 6755, 375, 444, 79, 1831, 1897, 614, 861,
          10, 10, 86, 33, 15, 88, 89, 993, NA)
df <- data_frame(X1 = colA, X2 = colB, X3 = colC, X4 = colD, X5 = colE, X6 = colF)
head(df)
```


The important thing here is to describe explicitly the position of each cell-value that belong to the spreadsheet into the R's data frame structure--*in row* $i$ *and column* $j$ *you find the cell-value* $x_{i,j}$--and the following function, **written and used by David Robinson in his answer**, reorganize data accordingly.

```{r}
library(dplyr)
library(tidyr)
tidy_excel <- function(x) {
  # x is a data imported from an excel file with readxl::read_excel
  #   function.
  # Return a data frame with the coordinate view representation.
  x %>% 
    setNames(seq_len(ncol(x))) %>% 
    mutate(row = row_number()) %>% 
    tidyr::gather(column, value, -row) %>% 
    mutate(column = as.integer(column)) %>% 
    group_by(row) %>% 
    filter(!all(is.na(value))) %>% 
    group_by(column) %>% 
    filter(!all(is.na(value))) %>% 
    ungroup() %>% 
    arrange(column, row)
}

tidy_excel(df)
```


As you can observe, the cells of the spreadsheet are melted into one column (*value*) and two new index-variables are created, one by each dimension (*row* and *column*), to mapping a cell coordinate of the spreadsheet with his content. This is the reason of the section title part *"coordinate-view"*.

A legitimate question now is **why can be useful a coordinate-view? What is the advantage with the original form?**

> **A:** It's possible to take an advantage to manipulate data based on his template structure and find regions with relevant data in the spreadsheet using filter operations over rows and columns. 

In fact, `tidy_excel` apart of reshape data into the coordinate-view is also an example of this. Pay attention to the following code snippet from the function definition of `tidy_excel`.


```{r, eval = FALSE}
# ...from the definition of tidy_excel
  group_by(row) %>%  # group by row index
  filter(!all(is.na(value))) %>%  # discard empty rows (row-groups that contain only NA values)
  group_by(column) %>%  # group by col index
  filter(!all(is.na(value))) # discard empty cols (col-groups that contain only NA values)
```

We can give more life to the above answer with an example. Imagine we want to know the index of the columns that their name contains the word "Weapon" and years between "2004" and "2008" from the previous table. In this example, the data is very small and you can know the answer just by looking. But suppose that we have multiple spreadsheets like this containing different period of years. In that case it would be useful a way to identify the row-index that contains the column names and deal with the particularities of each spreadsheet.

```{r}
# the column names are in the 4 row-index
df

# identify keywords in the column names
column_names_pattern <- "Weapons|2004|2005|2006|2007|2008"

# filter the coordinate view of data based on the previous key-words
df %>% 
  tidy_excel %>%  # apply the coordinate-view
  filter(stringr::str_detect(value, column_names_pattern))  # use regex to detect cell that satisfy a pattern
```


The above code is just a logical filter operation over data. In other words, we give to the `dplyr::filter` function a logical vector (*TRUE / FALSE*) of the same length as number of rows and return only rows in which the index is *TRUE*. The coordinate view allow us to know directly in which row and column the relevant values are located in the spreadsheet. If we pay atention to the row variable you can see that the header content is located in the fourth row.

How we generate the logical vector with `stringr::str_detect` is the following step.

### Step 2: Make your shot with regex

> *"A regular expression (or just regex) is a sequence of character that define a search pattern."* (<a href="https://en.m.wikipedia.org/wiki/Regular_expression" target="_blank">Wikipedia</a>)
 

### A data wrangling case with excel files

This is a short case that deal with the problem described previously, we have many spreadsheet files and we want to put all together into one dataset. The data has information of all corporate bonds issues and current debt status of bonds on the chilean local fixed-income market. This information is public on the website of the "Comisión para el mercado financiero (CMF)" to download in xlsx files [here](http://www.cmfchile.cl/portal/estadisticas/606/w3-propertyvalue-20153.html).

![](img/xlsx_file_case-min.png#center){ width=100% }

The goal is to use the insights that were highlighted from David Robinson answer to make a spreadsheet into a tidy data and to generalize for when we have more than one file. An important feature of this set of files is that it contains information of the same phenomenon (bond issued) on different periods of time.

We have three xlsx files corresponding to information of the years 2013, 2014 and 2015. Each of these files contain more than one sheet because data is reported in a monthly frequency. So we use the `purrr package` to apply operations over many sheets at the same time (*if you don't know how purrr works check* [Jenny Bryan's tutorial](https://jennybc.github.io/purrr-tutorial/))

The first thing to do is explore each file and the number of sheet it has using `readxl::excel_sheets`.

```{r}
# first we load the packages used during the analysis
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
library(readxl)


files <- list.files("./post_data/a_data_wrangling_case_with_spreadsheets_using_r", full.names = TRUE)
files

sheet_per_file <- map(files, readxl::excel_sheets)
sheet_per_file
```

Now we need at least two arguments for `read_excel` to bring each excel sheet into R:

1. *path to the xlsx file* 
2. *name of the sheet to read*. 

A possible way to build this pair of arguments are the **cross product** between the variables `files` and `sheet_per_file`. But before that, the cruel world gives us an inconvenient, the first file contain more than twelve sheets and, as we know, a year has twelve months...so there are some useless sheets (*Hoja13*).

```{r}
# for each sheet name vector we discard the names that aren't relevant
pattern <- c("Enero|Febrero|Marzo|Abril|Mayo|Junio|Julio|Agosto|Septiembre|Octubre|Noviembre|Diciembre")
relevant_sheet_name <- map(sheet_per_file, 
                           ~ .x[str_detect(.x, pattern)])
```

We continue with the cross product (`purrr::cross`) and we obtain a nested list of 36 elements (`arg_list`), in which each element of the list has the two arguments necessaries to read each sheet. Below you can see the first two pairs of arguments to feed `read_excel`.

```{r}
arg_list <- map2(files, relevant_sheet_name, ~ list(.x, .y))
arg_list <- map(arg_list, purrr::cross)
arg_list <- purrr::flatten(arg_list)
head(arg_list, n = 2)
```

Now two things will happen, first we will iterate a list (`arg_list`) over a function (guess which one?...`read_excel`) to read each sheet and store in a list called `raw_data`. Then we will give a name to each element (*a dataframe*) of the output list `raw_data`. We need this metainformation to identify each dataframe with his corresponding file and sheet. By using *regex* we extract from each `arg_list` element the <u>year</u> and <u>month</u> from the first and second argument respectively. Look how the sheets are in `raw_data`, you can see the name after the dollar sign.

```{r}
raw_data <- invoke_map(read_excel, arg_list)

# add metainformation from the names of the files and sheet into the name
# of elemenet list
list_names <- map(arg_list, ~ paste(str_extract(.x[[1]], "[0-9]+"), 
                                    str_extract(.x[[2]], "[aA-zZ]+"), sep = "_"))

names(raw_data) <- list_names
head(raw_data, n = 2)
```

It is necessary to clean `raw_data` before we collapse it into one dataset. We want to discard rows with meta-information and select all the columns until column *"Valor Par (en miles de $)"*. So first we need to detect the row that contain the column names of the table in the spreadsheet, then the name of our last column, and finally the last row that contain values (rewrite this part of the paragraph). We can do this by applying regular expressions on coordinate view and encapsulate all this process into the function `clean_spreadsheet` to apply it simultaneously over each dataframe. 

```{r}
clean_spreadsheet <- function(df) {
  tbl <- tidy_excel(df)
  last_col <- tbl %>% 
    filter(str_detect(value, "[Vv][Aa][Ll][Oo][Rr] [Pp][Aa][Rr]")) %>% 
    select(column) %>% 
    max()
  last_row <- tbl %>% 
    filter(str_detect(value, "[Tt][Oo][Tt][Aa][Ll]")) %>% 
    select(row) %>% 
    min()
  first_row <- tbl %>%
    filter(str_detect(value, "[Rr][Uu][Tt]")) %>% 
    select(row) %>% 
    min()
  sub_tbl <- tbl %>%
    filter(row <= last_row - 1, column <= last_col,
           row >= first_row + 1) %>% 
    tidyr::spread(column, value) %>% 
    select(-row)

}

clean_data <- purrr::map(raw_data, clean_spreadsheet)
head(clean_data, n = 2)
```

Are the different dataframes of the same dimensions? We are interested that each dataframe has the same number of column because they need to have the same type of information, evidently they could have different number of rows (more or less bonds as observations).

```{r}
purrr::map(clean_data, ~ ncol(.)) %>% 
  unlist() %>% 
  table()
```

Approximately a 86% of the data has 19 columns, then it is make sense to focus on the case of with 19 columns. The other two cases require to compare columns to find the missing or extra column but this extends the scope of this post (focus on the big picture).

```{r}
data <- clean_data %>% 
            purrr::keep(~dim(.)[2] == 19) %>% 
            purrr::imap(~ mutate(.x, periodo_reporte = .y)) %>% 
            bind_rows() %>% 
            mutate(mes = str_extract(periodo_reporte, "[^_\\a]+$"),
                   anho = str_extract(periodo_reporte, "^[^_\\D]+")) %>% 
            select(anho, mes, `1`:`19`)

col_names <- c("anho", "mes", "rut", "dv", "sociedad", "tipo_bono", "num_inscripccion", "fecha_inscripccion",
  "unidad", "monto_inscrito_miles", "serie", "tasa_emision", "objetivo_emision1",
  "objetivo_emision2", "objetivo_emision3", "anhos_vencimiento", "valor_nominal_inicial", "valor_nominal_vigente",
  "valor_nominal_reaj", "int_dev_no_pagado", "valor_par")

names(data) <- col_names
data
```

The result is a data with 21 columns and 33261 observations. We are almost ready, but before it's over, put your attention into the variable `fecha_inscripccion`(This variable's name is translate to "inscription date").

```{r}
data %>% 
  select(fecha_inscripccion)
```

These numbers doesn't look as a date but actually it's how excel stores the dates. David Robinson use the following function to clean these values. 

```{r}
convert_excel_date <- function(x) {
  # created by David Robinson
  result <- as.Date("1900-01-01") + as.numeric(x) - 2 
  ifelse(is.na(result), x, as.character(result))
}

data <- data %>% 
            mutate(fecha_inscripccion = lubridate::ymd(convert_excel_date(fecha_inscripccion)))

data %>% 
  select(fecha_inscripccion)
```

Finally we have a clean dataset ready for you to analyze. You can take a look to a few observations!

```{r, echo = FALSE}
data %>% 
  filter(!is.na(valor_par)) %>% 
  slice(1:25) %>% 
  knitr::kable("html") %>% 
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 14) %>%
  kableExtra::scroll_box(width = "100%", height = "200px")
```

```{r, eval = FALSE, echo = FALSE}
library(ggplot2)

data %>% 
  group_by(anho) %>% 
  summarise(total_par_MM = sum(as.numeric(valor_par), na.rm = TRUE) / 1000000) %>% ggplot(aes(x = factor(anho), y = total_par_MM)) +
  geom_bar(stat = "identity")
```


