---
title: A data wrangling case with spreadsheets using R
author: Cristóbal Alcázar
date: '2018-01-16'
slug: a-data-wrangling-case-with-r
categories: [R]
tags: [R, Data-Science]
comments: yes
showcomments: yes
showpagemeta: yes
---

```{r, message=FALSE, echo=FALSE}
library(dplyr)
```

![](img/spreadsheet_shock.jpg#center){ width=60% }

## Introduction 

Data wrangling is the process of mapping raw data to a format more suitable to model, visualize or in general to work it. This process is usually thought as an early stage of the data analysis pipeline (*e.g. I need a nice table to start the analysis*). But the skills involved in the process are useful whenever you need to reshape or look your data from different perspectives, or when you need to adapt some new tool that required a specific format. The reflexion:

>*data wrangling skills have more to say that only in an initial stage of the analysis, the latter can be augmented with these skills, enabling more flexibility and expressiveness in the whole pipeline.*

But the reason of this post is show you the essential of data wrangling: dominate data or take control of it. As you know, data in real life aren't always friendly and is not uncommon to found problems relative to the shape or format. An one case that contain both of these problem are the spreadsheet data.


(Just) to be fair not all the spreadsheets are a problem. Let me clarify what I refer by "problem", *the mess of more than one table per sheet like islands floating in a ocean of blank cells with clouds of metainformation headers relative to the tables...ok.*

You can look by yourself what I try to say in the following magistral example, but first some context. Below of this paragraph you can find a series of tweets belonged to very well known data scientists (*if you don't hear about them, I strongly recommend that you follow them*). The discussion is about a challenge to turn <a href="https://t.co/v9ucC0Vj2t" target="_blank">this spreadsheet</a> (a xlsx file) into a tidy data--*the grace is that the spreadsheet suffer the problems that I tried to describe*--and <a href="http://varianceexplained.org/" target="_blank">David Robinson (*the challenged*) </a>deal with the problem in  <a href="http://rpubs.com/dgrtwo/tidying-enron" target="_blank">a very simple and fluently way</a>.

`r blogdown::shortcode('tweet', '717815339480977410#center')`

`r blogdown::shortcode('tweet', '718170667573579776')`

In the rest of the post I will dedicate to highlight some steps of the David Robinson answer to the tweet challenge, and then apply the insights on a case of data contained in multiple "xlsx"" files with many sheets.

## The highlights steps

### Step 1: The coordinate-view

Consider the next innocent table as an example.

![](img/murdervictims.png#center){ width=65% }

It's easy to import the above spreadsheet with the `readxl::read_excel` function and obtain a rectangular table in R, filled with missing values (NA) instead of blank cells. You can take a look of the first six rows.

```{r, eval=FALSE}
library(readxl)
df <- readxl::read_excel("./innocent_table.xlsx")
df
```

```{r, echo=FALSE}
colA <- c("Expanded Homicide Data Table 8", "Murder Victims", "by Weapon, 2004-2008", "Weapons", "Total", "Total firearms:", "Handguns", "Rifles", "Shotguns", "Other guns", "Firearms, type not stated", "Knives or cutting instruments", "Blunt objetcs (clubs, hammers, etc)", "Personal weapons (hands, fist, feet, etc)1", "Poison", "Explosives", "Fire", "Narcotics", "Drowning", "Strangulation", "Asphyxiation", "Other weapons or weapons not stated", "1 Pushed is included in personal weapons")
colB <- c(NA, NA, NA, 2004, 14210, 9385, 7286, 403, 507, 117, 1072, 1866, 667, 
          943, 13, 1, 118, 80, 16, 156, 109, 856, NA)
colC <- c(NA, NA, NA, 2005, 14965, 10158, 7565, 445, 522, 138, 1488, 1920, 608,
          905, 9, 2,  125, 46, 20, 118, 96, 958, NA)
colD <- c(NA, NA, NA, 2006, 15087, 10225, 7836, 438, 490, 107, 1354, 1830, 618, 
          841, 12, 1, 117, 48, 12, 137, 106, 1140, NA)
colE <- c(NA, NA, NA, 2007, 14916, 10129, 7398, 453, 457, 116, 1705, 1817, 647, 869, 
          10, 1, 131, 52, 12, 134, 109, 1005, NA)
colF <- c(NA, NA, NA, 2008, 14180, 9484, 6755, 375, 444, 79, 1831, 1897, 614, 861,
          10, 10, 86, 33, 15, 88, 89, 993, NA)
df <- data_frame(X1 = colA, X2 = colB, X3 = colC, X4 = colD, X5 = colE, X6 = colF)
head(df)
```



The importance of this step is to describe explicitly the position of each cell-value of the spreadsheet into the R's data frame structure--*in row* $i$ *and column* $j$ *you found the value* $x_{i,j}$--and the following function, created and used by David Robinson in his answer, reorganize the data accordingly.

```{r}
library(dplyr)
library(tidyr)
tidy_excel <- function(x) {
  # x is a data imported from an excel file with readxl::read_excel
  #   function.
  # Return a data frame with the coordinate view representation.
  x %>% 
    setNames(seq_len(ncol(x))) %>% 
    mutate(row = row_number()) %>% 
    tidyr::gather(column, value, -row) %>% 
    mutate(column = as.integer(column)) %>% 
    group_by(row) %>% 
    filter(!all(is.na(value))) %>% 
    group_by(column) %>% 
    filter(!all(is.na(value))) %>% 
    ungroup() %>% 
    arrange(column, row)
}

tidy_excel(df)
```


As you can observe, the cells of the spreadsheet are melted into one column (*value*) and two new index-variables are created, one by each dimension (*row* and *column*), to mapping a cell coordinate of the spreadsheet with his content. This is the reason of the section title part *"coordinate-view"*.

The legitimate question now is **why can be useful the coordinate-view? What is the advantage with the previous form?**

> **A:** *It's possible to take advantage of manipulate data based on the physical position of the template using operations over columns and rows. In fact,* `tidy_excel` *apart of reshape data into the coordinate-view is also an example of this. Pay attention to the following code snippet from the function definition of* `tidy_excel`.

```{r, eval = FALSE}
# ...from the definition of tidy_excel
  group_by(row) %>%  # group by row index
  filter(!all(is.na(value))) %>%  # discard empty rows (row-groups that contain only NA values)
  group_by(column) %>%  # group by col index
  filter(!all(is.na(value))) # discard empty cols (col-groups that contain only NA values)
```

We can give more life to the above answer with an example. Imagine that we want to know the index of a header that contain "weapon" and years (2004...2008) of the previous table. Indeed like the data is very small you can know the answer just by looking. But suppose that we have multiple spreadsheets like that and the row index header can vary between the files. In this case it would be useful a way to identify the row-index that contain the header and deal with the particularities of each spreadsheet.

```{r}
# 4-row
df

# identify the header table in the coordinate-view representation
header_names_pattern <- "Weapons|2004|2005|2006|2007|2008"

# filter the column value based on a string pattern
df %>% 
  tidy_excel %>%  # apply the coordinate-view
  filter(stringr::str_detect(value, header_names_pattern))
```


The above code is just a logical filter operation over the data. In other words, we give to the `dplyr::filter` function a logical vector (*TRUE / FALSE*) of the same length that the rows of the data and return only the rows in which the index is *TRUE*. The coordinate view allow us to know directly in which row and column the relevant values are located in the spreadsheet. If we pay atention to the row variable you can see that the header content is located in the fourth row.

How we generate the logical vector with `stringr::str_detect` is the following step.


### Step 2: Make your shot with regex

> *"A regular expression (or just regex) is a sequence of character that define a search pattern."* (<a href="https://en.m.wikipedia.org/wiki/Regular_expression" target="_blank">Wikipedia</a>)
 

## The Case


**Describe better the problem and the goal**

The aim of this case is use the insights of the highlights step and extract tables from three excel files (*xlsx*), Each of these correspond to a year files contain multiple sheets with the same information but different months.

- Use the name of the files and sheet as metadata information to then identify each
data
- Each excel sheet into one data frame


```{r}
# first we load the packages used during the analysis
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
library(readxl)


files <- list.files("./post_data/a_data_wrangling_case_with_spreadsheets_using_r", full.names = TRUE)
files

sheet_per_file <- map(files, readxl::excel_sheets)
sheet_per_file
```

We need a pair of arguments for `read_excel` to read each sheet:

- *path to the xlsx file* 

- *the name of the sheet*. 

A possible way to build these pair of arguments are the **cross product** between the variables `files` and `sheet_per_file`. But before that the crude world gives us an inconvenient, the first file contain more than twelve sheet and, as we know, a year has twelve months..so a sheet is being a fly (*Hoja13*).

```{r}
# for each sheet name vector we discard the names that aren't relevant
pattern <- c("Enero|Febrero|Marzo|Abril|Mayo|Junio|Julio|Agosto|Septiembre|Octubre|Noviembre|Diciembre")
relevant_sheet_name <- map(sheet_per_file, 
                           ~ .x[str_detect(.x, pattern)])
```

We continue with the cross product (`purrr::cross`)...and we obtain a nested list of 36 element (`arg_list`), in which each element is a list of length two with the necessaries arguments to read each sheet. Below you can see the first two pairs of argument to feed `read_excel`.

```{r}
arg_list <- map2(files, relevant_sheet_name, ~ list(.x, .y))
arg_list <- map(arg_list, purrr::cross)
arg_list <- purrr::flatten(arg_list)
head(arg_list, n = 2)
```

Now two things will happen, first I will iterate a list (`arg_list`) over a function, guess which one?...`read_excel` to read each sheet and store in a list called `raw_data`. Then I will give a name to each element (*dataframe*) of the output list `raw_data`. We need the metainformation to identify each dataframe with his corresponding file and sheet. So using *regex* we extract from each `arg_list` element the <u>year</u> and <u>month</u> from the first and second argument respectively. Look how the sheets are in `raw_data`, you can see the name after the dollar sign.
```{r}
raw_data <- invoke_map(read_excel, arg_list)

# add metainformation from the names of the files and sheet into the name
# of elemenet list
list_names <- map(arg_list, ~ paste(str_extract(.x[[1]], "[0-9]+"), 
                                    str_extract(.x[[2]], "[aA-zZ]+"), sep = "_"))

names(raw_data) <- list_names
head(raw_data, n = 2)
```

