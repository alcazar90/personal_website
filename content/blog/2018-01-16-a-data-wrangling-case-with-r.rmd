---
title: A data wrangling case with spreadsheets using R
author: Cristóbal Alcázar
date: '2018-01-16'
slug: a-data-wrangling-case-with-r
categories: [R]
tags: [R, Data-Science]
comments: yes
showcomments: yes
showpagemeta: yes
---
```{r set-options, echo = FALSE, cache = FALSE}
options(width = 50)
```

```{r, message=FALSE, echo=FALSE}
library(dplyr)
```

![](img/spreadsheet_shock-min.jpg#center){ width=60% }


A typical real world situation is the problem of collect data from a set of files and structured into one dataset. "Why data is separate?" you can think, well this could response to many reasons. For example, some phenomenon are measure along the time and you can only access to data by period or a bussiness is collecting some information of his units and the origin process of data was not centralized. So you end up with one file per unit in your hands.

Another typical situation is that data is share or is avaiable in excel files. That is because excel is the main used analytical tool in the world (I guess!) and many of us we didn't have a class of best practices on how share and document data. The problem with excel files are the "spreadsheet intrinsic issues". What is that?...is just a fancy way to call the messy format problems that are common to find in spreadsheet files like multiple header levels, more than one table per sheet, different data types values in same column, and the list goes on.

You can look by yourself what I try to say in the following magistral example, but first some context. Below of this paragraph you can find a series of tweets belonged to very well known data scientists. The discussion is about a challenge to turn <a href="https://t.co/v9ucC0Vj2t" target="_blank">this spreadsheet</a> (a xlsx file) into a tidy data and <a href="http://varianceexplained.org/" target="_blank">David Robinson (*the challenged*) </a>deal with the problem in  <a href="http://rpubs.com/dgrtwo/tidying-enron" target="_blank">a very simple and fluently way</a>.

`r blogdown::shortcode('tweet', '717815339480977410#center')`

`r blogdown::shortcode('tweet', '718170667573579776')`

The rest of the post is dedicated to highlight some steps of David Robinson answer to the tweet challenge, and then apply the insights on a case of data contained in multiple "xlsx"" files with many sheets.

### Step 1: The coordinate-view

Consider the next innocent table as an example.

![](img/murdervictims.png#center){ width=65% }

It's easy to import the above spreadsheet with the `readxl::read_excel` function and obtain a rectangular table in R, filled with missing values (NA) instead of blank cells. You can take a look of the first six rows.

```{r, eval=FALSE}
library(readxl)
df <- readxl::read_excel("./innocent_table.xlsx")
df
```

```{r, echo=FALSE}
colA <- c("Expanded Homicide Data Table 8", "Murder Victims", "by Weapon, 2004-2008", "Weapons", "Total", "Total firearms:", "Handguns", "Rifles", "Shotguns", "Other guns", "Firearms, type not stated", "Knives or cutting instruments", "Blunt objetcs (clubs, hammers, etc)", "Personal weapons (hands, fist, feet, etc)1", "Poison", "Explosives", "Fire", "Narcotics", "Drowning", "Strangulation", "Asphyxiation", "Other weapons or weapons not stated", "1 Pushed is included in personal weapons")
colB <- c(NA, NA, NA, 2004, 14210, 9385, 7286, 403, 507, 117, 1072, 1866, 667, 
          943, 13, 1, 118, 80, 16, 156, 109, 856, NA)
colC <- c(NA, NA, NA, 2005, 14965, 10158, 7565, 445, 522, 138, 1488, 1920, 608,
          905, 9, 2,  125, 46, 20, 118, 96, 958, NA)
colD <- c(NA, NA, NA, 2006, 15087, 10225, 7836, 438, 490, 107, 1354, 1830, 618, 
          841, 12, 1, 117, 48, 12, 137, 106, 1140, NA)
colE <- c(NA, NA, NA, 2007, 14916, 10129, 7398, 453, 457, 116, 1705, 1817, 647, 869, 
          10, 1, 131, 52, 12, 134, 109, 1005, NA)
colF <- c(NA, NA, NA, 2008, 14180, 9484, 6755, 375, 444, 79, 1831, 1897, 614, 861,
          10, 10, 86, 33, 15, 88, 89, 993, NA)
df <- data_frame(X1 = colA, X2 = colB, X3 = colC, X4 = colD, X5 = colE, X6 = colF)
head(df)
```


The important thing here is to describe explicitly the position of each cell-value that belong to the spreadsheet into the R's data frame structure--*in row* $i$ *and column* $j$ *you found the cell-value* $x_{i,j}$--and the following function, **written and used by David Robinson in his answer**, reorganize data accordingly.

```{r}
library(dplyr)
library(tidyr)
tidy_excel <- function(x) {
  # x is a data imported from an excel file with readxl::read_excel
  #   function.
  # Return a data frame with the coordinate view representation.
  x %>% 
    setNames(seq_len(ncol(x))) %>% 
    mutate(row = row_number()) %>% 
    tidyr::gather(column, value, -row) %>% 
    mutate(column = as.integer(column)) %>% 
    group_by(row) %>% 
    filter(!all(is.na(value))) %>% 
    group_by(column) %>% 
    filter(!all(is.na(value))) %>% 
    ungroup() %>% 
    arrange(column, row)
}

tidy_excel(df)
```


As you can observe, the cells of the spreadsheet are melted into one column (*value*) and two new index-variables are created, one by each dimension (*row* and *column*), to mapping a cell coordinate of the spreadsheet with his content. This is the reason of the section title part *"coordinate-view"*.

A legitimate question now is **why can be useful a coordinate-view? What is the advantage with the original form?**

> **A:** It's possible to take advantage of manipulate data based on his template structure and found regions with relevant data in the spreadsheet using filter operations over rows and columns. 

In fact, `tidy_excel` apart of reshape data into the coordinate-view is also an example of this. Pay attention to the following code snippet from the function definition of `tidy_excel`.


```{r, eval = FALSE}
# ...from the definition of tidy_excel
  group_by(row) %>%  # group by row index
  filter(!all(is.na(value))) %>%  # discard empty rows (row-groups that contain only NA values)
  group_by(column) %>%  # group by col index
  filter(!all(is.na(value))) # discard empty cols (col-groups that contain only NA values)
```

We can give more life to the above answer with an example. Imagine we want to know the index of column names that contain the word "Weapon" and years from 2004 to 2008 of the previous table. Indeed like the data is very small you can know the answer just by looking. But suppose that we have multiple spreadsheets like this containing different period of years. In this case it would be useful a way to identify the row-index that contain the column names and deal with the particularities of each spreadsheet.

```{r}
# the column names are in the 4 row-index
df

# identify keywords in the column names
column_names_pattern <- "Weapons|2004|2005|2006|2007|2008"

# filter the coordinate view of data based on the previous key-words
df %>% 
  tidy_excel %>%  # apply the coordinate-view
  filter(stringr::str_detect(value, column_names_pattern))  # use regex to detect cell that satisfy a pattern
```


The above code is just a logical filter operation over data. In other words, we give to the `dplyr::filter` function a logical vector (*TRUE / FALSE*) of the same length that rows of the data and return only rows in which the index is *TRUE*. The coordinate view allow us to know directly in which row and column the relevant values are located in the spreadsheet. If we pay atention to the row variable you can see that the header content is located in the fourth row.

How we generate the logical vector with `stringr::str_detect` is the following step.

### Step 2: Make your shot with regex

> *"A regular expression (or just regex) is a sequence of character that define a search pattern."* (<a href="https://en.m.wikipedia.org/wiki/Regular_expression" target="_blank">Wikipedia</a>)
 

### A data wrangling case with excel files

This is a short case that deal with the problem described previously, we have many spreadsheet files and we want to put all together into one dataset. The data has information of all corporate bonds issues and current debt status of bonds on the chilean local fixed-income market. This information is public available on the website of the "Comisión para el mercado financiero (CMF)" to download in xlsx files [here](http://www.cmfchile.cl/portal/estadisticas/606/w3-propertyvalue-20153.html).

![](img/xlsx_file_case-min.png#center){ width=100% }

The goal is to use the insights that we highlight from David Robinson answer to make a spreadsheet into a tidy data and generalize for the situation that we have more than one file. An important feature of this set of files is that contain information of the same phenomenon (bond issues) by different periods of time.

We have three xlsx files corresponding to information of 2013, 2014 and 2015. Each of these files contain more than one sheet because data is reported in a monthly frequency. So we use the `purrr package` to apply operations over many sheets at the same time (*if you don't know purrr check* [Jenny Bryan's tutorial](https://jennybc.github.io/purrr-tutorial/))

A first thing to do is explore each file and the number of sheet that contain using `readxl::excel_sheets`.

```{r}
# first we load the packages used during the analysis
library(dplyr)
library(tidyr)
library(stringr)
library(purrr)
library(readxl)


files <- list.files("./post_data/a_data_wrangling_case_with_spreadsheets_using_r", full.names = TRUE)
files

sheet_per_file <- map(files, readxl::excel_sheets)
sheet_per_file
```

Now at least we need two arguments for `read_excel` to bring each excel sheet into R:

1. *path to the xlsx file* 
2. *name of the sheet to read*. 

A possible way to build these pair of arguments are the **cross product** between the variables `files` and `sheet_per_file`. But before that, the crude world gives us an inconvenient, the first file contain more than twelve sheet and, as we know, a year has twelve months..so a sheet is being a fly (*Hoja13*).

```{r}
# for each sheet name vector we discard the names that aren't relevant
pattern <- c("Enero|Febrero|Marzo|Abril|Mayo|Junio|Julio|Agosto|Septiembre|Octubre|Noviembre|Diciembre")
relevant_sheet_name <- map(sheet_per_file, 
                           ~ .x[str_detect(.x, pattern)])
```

We continue with the cross product (`purrr::cross`) and we obtain a nested list of 36 element (`arg_list`), in which each element of the list has a length of two with it necessary arguments to read each sheet. Below you can see the first two pairs of argument to feed `read_excel`.

```{r}
arg_list <- map2(files, relevant_sheet_name, ~ list(.x, .y))
arg_list <- map(arg_list, purrr::cross)
arg_list <- purrr::flatten(arg_list)
head(arg_list, n = 2)
```

Now two things will happen, first we will iterate a list (`arg_list`) over a function, guess which one?...`read_excel` to read each sheet and store in a list called `raw_data`. Then we will give a name to each element (*a dataframe*) of the output list `raw_data`. We need this metainformation to identify each dataframe with his corresponding file and sheet. So using *regex* we extract from each `arg_list` element the <u>year</u> and <u>month</u> from the first and second argument respectively. Look how the sheets are in `raw_data`, you can see the name after the dollar sign.

```{r}
raw_data <- invoke_map(read_excel, arg_list)

# add metainformation from the names of the files and sheet into the name
# of elemenet list
list_names <- map(arg_list, ~ paste(str_extract(.x[[1]], "[0-9]+"), 
                                    str_extract(.x[[2]], "[aA-zZ]+"), sep = "_"))

names(raw_data) <- list_names
head(raw_data, n = 2)
```

It is necessary to clean before to collapse `raw_data` into one dataset. We want to discard rows with meta-information and select all the columns until column *"Valor Par (en miles de $)"*. So first we need to detect the row that contain the column names of the table in the spreadsheet, then the name of our last column, and finally the last row that contain values. We can do this by applying regular expressions on coordinate view. Another point is to  encapsulate all this process into the function `clean_spreadsheet` to apply simultaneously over each dataframe. 

```{r}
clean_spreadsheet <- function(df) {
  tbl <- tidy_excel(df)
  last_col <- tbl %>% 
    filter(str_detect(value, "[Vv][Aa][Ll][Oo][Rr] [Pp][Aa][Rr]")) %>% 
    select(column) %>% 
    max()
  last_row <- tbl %>% 
    filter(str_detect(value, "[Tt][Oo][Tt][Aa][Ll]")) %>% 
    select(row) %>% 
    min()
  first_row <- tbl %>%
    filter(str_detect(value, "[Rr][Uu][Tt]")) %>% 
    select(row) %>% 
    min()
  sub_tbl <- tbl %>%
    filter(row <= last_row - 1, column <= last_col,
           row >= first_row + 1) %>% 
    tidyr::spread(column, value) %>% 
    select(-row)

}

clean_data <- purrr::map(raw_data, clean_spreadsheet)
head(clean_data, n = 2)
```

Are dataframes of the same dimensions? We are interested in that each data has the same number of column because evidently could have different rows (more or less bonds as observations).

```{r}
purrr::map(clean_data, ~ ncol(.)) %>% 
  unlist() %>% 
  table()
```

Approximately a 86% of the data has 19 columns, then is make sense to focus on the case of 19 columns.

```{r}
data <- clean_data %>% 
            purrr::keep(~dim(.)[2] == 19) %>% 
            purrr::imap(~ mutate(.x, periodo_reporte = .y)) %>% 
            bind_rows() %>% 
            mutate(mes = str_extract(periodo_reporte, "[^_\\a]+$"),
                   anho = str_extract(periodo_reporte, "^[^_\\D]+")) %>% 
            select(anho, mes, `1`:`19`)

col_names <- c("anho", "mes", "rut", "dv", "sociedad", "tipo_bono", "num_inscripccion", "fecha_inscripccion",
  "unidad", "monto_inscrito_miles", "serie", "tasa_emision", "objetivo_emision1",
  "objetivo_emision2", "objetivo_emision3", "anhos_vencimiento", "valor_nominal_inicial", "valor_nominal_vigente",
  "valor_nominal_reaj", "int_dev_no_pagado", "valor_par")

names(data) <- col_names
data
```

If we take a look to the variable `fecha_inscripccion`.

```{r}
data %>% 
  select(fecha_inscripccion)
```

These numbers doesn't look as a date but actually is how excel stores the dates. David Robinson use the following function to clean these values. 

```{r}
convert_excel_date <- function(x) {
  # created by David Robinson
  result <- as.Date("1900-01-01") + as.numeric(x) - 2 
  ifelse(is.na(result), x, as.character(result))
}

data <- data %>% 
            mutate(fecha_inscripccion = lubridate::ymd(convert_excel_date(fecha_inscripccion)))

data %>% 
  select(fecha_inscripccion)
```

We can take a look to the first ten observations.

```{r, echo = FALSE}
data %>% 
  slice(1:10) %>% 
  knitr::kable("html") %>% 
  kableExtra::kable_styling(bootstrap_options = "striped", font_size = 14) %>%
  kableExtra::scroll_box(width = "100%", height = "200px")
```

