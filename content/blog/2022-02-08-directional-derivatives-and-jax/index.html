---
title: Directional derivatives and JAX
author: Crist√≥bal Alc√°zar
date: '2022-02-08'
draft: false
slug: []
categories: [JAX, AUTODIFF, PYTHON, MML, CALCULUS]
tags: [MML, CALCULUS, JAX]
comments: yes
showcomments: yes
showpagemeta: yes
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p><a href="https://colab.research.google.com/drive/1VD0QIfC-Q3WgmBPpgfAcF9zMZL_NAa5G?usp=sharing" target="_blank">
<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a></p>
<center>
<img src="/img/directional-derivative-post/directional_derivatives_setting.png">
</center>
<p><br></p>
<p>Directional derivatives are the conceptual tool to measure the effect on a
function by changing the input in any direction within the input space. It‚Äôs
possible to compute the directional derivatives using the jacobian-vector
product, implemented by the automatic differentiation JAX library.</p>
<p>Partial derivatives <span class="math inline">\(\partial f/ \partial x_i\)</span> give us the rate of change if
we slightly modify the <em>ith element</em> of the input vector <span class="math inline">\(\bf{x}\)</span> letting the
rest constant.</p>
<p><br></p>
<p><span class="math display">\[
\frac{\partial f}{\partial x_1} = lim_{h \to 0} \frac{f(x_1 + h, x_2, \dots, x_n) - f({\bf x})}{h}
\\
\vdots
\\
\frac{\partial f}{\partial x_n} = lim_{h \to 0} \frac{f(x_1, x_2, \dots, x_n + h) - f({\bf x})}{h}
\]</span></p>
<p><br></p>
<p>The above definition can be more compactly using vector notation.</p>
<p><br></p>
<p><span class="math display">\[
\nabla_{{\bf x}}f = \frac{\partial f}{\partial {\bf x}} = lim_{h \to 0} \frac{f({\bf x} + h{\bf e_i}) - f({\bf x})}{h}
\]</span>
<br>
The <span class="math inline">\(e_i\)</span> vector represents a unit vector in the direction of <span class="math inline">\(i\)</span>.</p>
<p>As you can see in the initial diagram, in a 2D input space, there are two
partial derivatives:</p>
<ul>
<li><span class="math inline">\(\partial f / \partial x\)</span>: computing parallel to the x-axis (<span class="math inline">\(e_1\)</span> <em>typical known as</em> <span class="math inline">\(\hat{i}\)</span>)</li>
<li><span class="math inline">\(\partial f / \partial y\)</span>: computing parallel to the y-axis (<span class="math inline">\(e_2\)</span> <em>typical known as</em> <span class="math inline">\(\hat{j}\)</span>).</li>
</ul>
<p>Computing derivatives using unit vectors such as <span class="math inline">\(e_i\)</span> give us the change of
<span class="math inline">\(f\)</span> on the direction on <span class="math inline">\(i\)</span>, or parallel to the <em>i-axis</em>. How can we compute
the derivative of <span class="math inline">\(f\)</span> given a slight nudge of the inputs in any arbitrary
direction?</p>
<p>Directional derivatives is a way to compute the rate of change on <span class="math inline">\(f\)</span> in the
direction of <span class="math inline">\({\bf v}\)</span>.</p>
<p><br>
<span class="math display">\[
\nabla_{{\bf v}}f = \frac{\partial f}{\partial {\bf x}} = lim_{h \to 0} \frac{f({\bf x} + h{\bf v}) - f({\bf x})}{h}
\]</span>
<br></p>
<p>Think as <span class="math inline">\({\bf v}\)</span> as a weighted vector of the <em>n-directions</em> of the input
space. We aren‚Äôt limited to the changes on <span class="math inline">\(f\)</span> in parallel directions in the input space.</p>
<p>We can compute directional derivatives using the dot product between the
jacobian vector (<span class="math inline">\(\nabla f\)</span>) and the vector <span class="math inline">\({\bf v}\)</span>. For instance, for a two-dimensional input space, <span class="math inline">\({\bf v}= (v_1, v_2)\)</span>:</p>
<p><span class="math display">\[\nabla f_{v} = \nabla f \cdot {\bf v} = \frac{\partial f}{\partial x_1} v_1 + \frac{\partial f}{\partial x_2}v_2\]</span></p>
<p>More general:</p>
<p><span class="math display">\[\nabla f_{{\bf v}}(p) = \nabla f(p) \cdot {\bf v} = \sum^{n}_{i=1} \frac{\partial f}{\partial x_i}(p) v_i\]</span></p>
<p>Let‚Äôs focus on computing the above using the function <code>jax.jvp</code>, which <code>jvp</code>
stands for the <em>jacobian-vector product</em>.</p>
<p>The function <code>jax.jvp</code> computes the directional derivative and whose arguments are:</p>
<ol style="list-style-type: decimal">
<li>A differentiable function <span class="math inline">\(f\)</span> for compute the jacobian <span class="math inline">\(\nabla f\)</span></li>
<li>A primal vector <span class="math inline">\({\bf p}\)</span> to evaluate the jacobian <span class="math inline">\(\nabla f(p)\)</span></li>
<li>A tangent vector <span class="math inline">\({\bf v}\)</span> which represent the direction in which we
want to calculate the derivative.</li>
</ol>
<p><code>jax.jvp</code> returns a tuple with <span class="math inline">\((f(p), \nabla f_{v}(p))\)</span></p>
<div id="example" class="section level3">
<h3>Example</h3>
<p>We compute the directional derivative of <span class="math inline">\(f(x, y)=x^2y\)</span> hand-coding all the
necessary elements and then check the results that give <code>jax.jvp</code>.</p>
<pre class="python"><code>def fun(x, y): return x**2 * y
def fun_dx(x, y): return 2*x*y
def fun_dy(x, y): return x**2</code></pre>
<p>We define the primal vector <span class="math inline">\({\bf p}\)</span> and the tangent vector <span class="math inline">\({\bf v}\)</span>
in which we want to compute the directional derivative.</p>
<pre class="python"><code>p = [1., 1.]
v = [1., 2.]</code></pre>
<p>Evaluate <span class="math inline">\(f(p)\)</span>:</p>
<pre class="python"><code># *n-list/n-tuple unpack the element e0, e1, ..., en
fun(*p)
&gt; 1.0</code></pre>
<p>Compute the directional derivative using the <code>fun_dx</code> and <code>fun_dy</code>.</p>
<pre class="python"><code>fun_dx(*p) * v[0] + fun_dy(*p) * v[1]
&gt; 4.0</code></pre>
<p>Now using <code>jax.jvp</code> we obtain the same results: <span class="math inline">\(f({\bf p})\)</span> and <span class="math inline">\(\nabla_{\bf v}f({\bf p})\)</span>.</p>
<pre class="python"><code>jax.jvp(fun, p, v)
&gt; (DeviceArray(1., dtype=float32, weak_type=True),
   DeviceArray(4., dtype=float32, weak_type=True))</code></pre>
<p>A surface plot will show the output space, and a contour plot the input space
of <span class="math inline">\(f(x,y)=x^2y\)</span>. We will compute the directional derivatives for three points
and their respective directional vectors.</p>
<center>
<img src="/img/directional-derivative-post/directional_plot.png">
</center>
<p>Look the directional vectors in the plot, or tangent vectors as JAX refers to
them, there are of different lengths. It‚Äôs important to remark that if we want
the <a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/directional-derivative-introduction?modal=1">‚Äúslope definition‚Äù</a> for directional derivatives we
need to transform <span class="math inline">\({\bf v}\)</span> in a unit length vector (divide the directional
derivative definition by <span class="math inline">\(||v||\)</span>). Remember that partial
derivatives are computing using unit vectors (<span class="math inline">\(e_i\)</span>).</p>
<p><span class="math display">\[
\nabla_{{\bf v}}f = \frac{\partial f}{\partial {\bf x}} = lim_{h \to 0} \frac{f({\bf x} + h{\bf v}) - f({\bf x})}{h||{\bf v}||}
\]</span></p>
<pre class="python"><code>primal_a = jnp.array([-5., 3.2])
primal_b = jnp.array([5., -3.2])
primal_c = jnp.array([0., 0.])
va = jnp.array([-7.5, 5.7])
vb = jnp.array([7.5, -5.7])
vc = jnp.array([-1.0, -0.7])
unit_va = va/va.dot(va)**.5
unit_vb = vb/vb.dot(vb)**.5
unit_vc = vc/vc.dot(vc)**.5
# computing with scaling the directionl vectors
_, slope_a = jax.jvp(fun, primal_a.tolist(), unit_va.tolist())
_, slope_b = jax.jvp(fun, primal_b.tolist(), unit_vb.tolist())
_, slope_c = jax.jvp(fun, primal_c.tolist(), unit_vc.tolist())
slope_a, slope_b, slope_c
&gt; (DeviceArray(40.60427, dtype=float32, weak_type=True),
   DeviceArray(-40.60427, dtype=float32, weak_type=True),
   DeviceArray(-0., dtype=float32, weak_type=True))</code></pre>
<ul>
<li>Point A (blue): the directional derivative is 40.6, making sense with the contour lines in front of point A. The surface starter to rise in the direction of <span class="math inline">\(\overrightarrow{v}_a\)</span>.</li>
<li>Point B (red): the function ùëì decrease in the direction pointing out the vector <span class="math inline">\(\overrightarrow{v}_b\)</span>, like the directional derivative, ùëì changes ‚àí40.6 regard slightly input variations across the directional vector. Notice that it has the same magnitude as the slope of point A but goes in the opposite direction; the surface plot shows how the function increases/decreases in the same proportion across its diagonals.</li>
<li>Point C (black): the surface is practically flat around the point (0,0). Notice that the directional derivative at <span class="math inline">\(\overrightarrow{v}_c\)</span> is 0.</li>
</ul>
</div>
