---
title: Taylor Approximation and JAX
author: Crist√≥bal Alc√°zar
date: '2022-01-13'
draft: false
toc: true
slug: []
categories: [JAX, AUTODIFF, PYTHON, MML, CALCULUS]
tags: [MML, CALCULUS, JAX]
comments: no
showcomments: yes
showpagemeta: yes
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">

</div>

<p><a href="https://colab.research.google.com/drive/1KDAbU3eW-fOxAYmp0eiQFbuuRmZARdqq?usp=sharing">
<img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a></p>
<center>
<img src="/img/taylor-post/portrait.png">
</center>
<div id="taylor-approximation-review" class="section level3">
<h3>1. Taylor Approximation Review</h3>
<p>Taylor‚Äôs series allows us to approximate a function ùëì as a polynomial, computed
using derivatives. In the extrema, if we used infinite coefficients, or up to times
that ùëì can differentiate, we ended up with a perfect approximation.</p>
<p><span class="math display">\[T_n(x):=\sum_{k=0}^{n}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k\]</span></p>
<p><span class="math display">\[T_1(x):= f(x_0) + f^{(1)}(x_0)(x-x_0)\]</span></p>
<p>Note: ùëì(ùëò) is ùëì differentiate k times, and ùëò=0 is ùëì itself.</p>
<p>Let‚Äôs code an example; I will replicate figure 5.4 from the <a href="https://mml-book.github.io/book/mml-book.pdf">Mathematics for Machine Learning</a> book.</p>
<p>We want to approximate the following function around <span class="math inline">\(x=0\)</span>:
<br>
<br>
<span class="math display">\[f(x) = sin(x) + cos(x)\]</span></p>
<pre class="python"><code>def fun(x): 
    return np.sin(x) + np.cos(x)</code></pre>
<center>
<img src="/img/taylor-post/example_5-4_00.png">
</center>
<p>That is like <span class="math inline">\(f\)</span> looks like around <span class="math inline">\(x=0\)</span> The task is to get an expression that
describes how <span class="math inline">\(f\)</span> varies around the <span class="math inline">\(x=0\)</span> neighbourhood. The most straightforward
way to achieve this is to remember that <span class="math inline">\(f&#39;(x)\)</span> is another function that gives
us the tangent line at point <span class="math inline">\(x\)</span>. We finish our task; we get an approximation
of <span class="math inline">\(f\)</span> just giving the equation of the tangent line at ùë•.
Knowing what‚Äôs the derivative of <span class="math inline">\(sin\)</span> and <span class="math inline">\(cos\)</span> plus the addition rule for
differentiating, we can compute this manually:</p>
<p><span class="math display">\[f&#39;(x)=cos(x)-sin(x)\]</span></p>
<pre class="python"><code># Evaluating f`at x=0
np.cos(0) - np.sin(0)
&gt; 1.0</code></pre>
<p>And we get the equation for the tangent line approximating at <span class="math inline">\(x_0=0\)</span>,
<span class="math inline">\(y=f(0)=1\)</span>, and <span class="math inline">\(m=f&#39;(x_0=0)=1\)</span>:</p>
<p><br>
<span class="math display">\[y - y_0 = m (x - x_0)\]</span>
<br>
<span class="math display">\[y = 1 + f&#39;(x_0)x\]</span>
<br>
<span class="math display">\[y = 1 + x\]</span></p>
<pre class="python"><code>taylor_approx(fun, approx_around = 0.0, num_coef=2)</code></pre>
<center>
<img src="/img/taylor-post/example_5-4_01.png">
</center>
<p>Notice that the tangent line is a pretty good approximation in the immediate space
around <span class="math inline">\(x=0\)</span>, but we want something that goes beyond our block. Our approximation
gets higher errors when we cross the street at the corner (look at <span class="math inline">\(x=2\)</span>!).</p>
<p>If we want to be famous at a scale, we need to improve our approximation. To do
that, we can improve how we deal with the curvature.</p>
<pre class="python"><code>taylor_approx(fun, approx_around = 0.0, num_coef=3, PLOT_COEF = (0,1,2))</code></pre>
<center>
<img src="/img/taylor-post/example_5-4_02.png">
</center>
<p>The green line does a better job approximating <span class="math inline">\(f\)</span> within -1 and 1 than the line.
It was intuitive to get an expression for the tangent line, not a quadratic one.
How do we get the equation that describes the green line?</p>
<p>Here is when Taylor‚Äôs polynomial series is pretty handy:</p>
<p><span class="math display">\[T_2(x):=\sum_{k=0}^{2}\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k\]</span></p>
<p>To our line equation, we need to add the last term describes in <span class="math inline">\(T_2\)</span>. To obtain
this term, we need to compute <span class="math inline">\(f&#39;&#39;\)</span>.</p>
<p>In this case, the second derivative is easily computable given that the
derivatives are also cyclical because of the nature of <span class="math inline">\(sin\)</span> and <span class="math inline">\(cos\)</span>:</p>
<p><span class="math inline">\(f&#39;&#39;=\frac{\partial^2f}{\partial x^2}=\frac{\partial^2}{\partial x^2}\big(sin(x) + cos(x)\big)\)</span></p>
<p><span class="math inline">\(f&#39;&#39;=-sin(x)-cos(x)\)</span></p>
<p>The quadratic approximation or the <em>second-order</em> Taylor approximation is:</p>
<p><span class="math display">\[T_2(x) = 1 + x +  \frac{f&#39;&#39;(x_0)}{2}x^2\]</span></p>
<pre class="python"><code># Evaluating f&#39;&#39; at x=0
-np.sin(0) - np.cos(0)
&gt; -1.0</code></pre>
<p><span class="math display">\[T_2(x) = 1 + x  -\frac{1}{2}x^2\]</span>
It makes sense with the above pictures because the coefficient accompanied by the quadratic term is negative,
and therefore we have a concave down curve. Like you can see.</p>
<pre class="python"><code>plt.plot(x_jnp, 1 + x_jnp - 0.5 * x_jnp ** 2, 
         color=&#39;forestgreen&#39;,
         linestyle=&#39;-&#39;)</code></pre>
<center>
<img src="/img/taylor-post/example_5-4_03.png">
</center>
<p>So we can continue repeating this process, adding more coefficients and getting
a more accurate approximation. Of course, at the cost of computing higher-order
derivatives.</p>
<p>The below image is the final reproduction of figure 5.4. Notice the
power of 10 Taylor coefficients (red curve); it approximate <span class="math inline">\(f\)</span> within the domain
interval -4 and 4 almost perfectly. Be cautious, the same that happens with the
<em>fifth-order</em> Taylor approximation (green curve), which distances from <span class="math inline">\(f\)</span> in both
lateral of the plot; it would happen to <span class="math inline">\(T_{10}\)</span> if we expand the x-domain region
in the plot.</p>
<pre class="python"><code>taylor_approx(fun, approx_around = 0.0, num_coef=11)</code></pre>
<center>
<img src="/img/taylor-post/example_5-4_04.png">
</center>
<p>Some thoughts about this section.</p>
<ol style="list-style-type: decimal">
<li><p>How can we differentiate any ùëì no matter its complexity without relying on
manual computations?</p></li>
<li><p>How can we express the differentiation operations in code?</p></li>
<li><p>How can we extend Taylor approximation to multivariate functions
(i.e.¬†<span class="math inline">\(f(x_1, \dots, x_n)\)</span>) and everything which involve gradients?</p></li>
</ol>
</div>
<div id="introducing-automatic-differentiation-with-jax" class="section level3">
<h3>2. Introducing Automatic Differentiation with JAX</h3>
<p><a href="https://jax.readthedocs.io/en/latest/index.html">JAX</a> is a python library that
combines the <code>numpy</code>‚Äôs interface, automatic differentiation capabilities, and
high-performance operations using XLA and GPU operations.</p>
<p>In this section, we will focus on the fundamentals of JAX to illustrate how to perform
automatic differentiation and understand how JAX operates at a high level.</p>
<ol style="list-style-type: decimal">
<li><code>jax.grad()</code>: given a function <span class="math inline">\(f(x)\)</span> implemented in code, it returns a function
for compute the gradient (<span class="math inline">\(f&#39;(x)\)</span>)</li>
<li><code>jax.vmap()</code>: vectorize a <code>jax.grad</code>‚Äôs function</li>
<li><code>jax.jit()</code>: accelerate a function computations using XLA</li>
</ol>
<p>Let‚Äôs start with an <a href="https://github.com/hips/autograd">example</a> used by the <code>autograd</code>
library, the predecessor of <code>JAX</code>: differentiate the hyperbolic tangent function.</p>
<p>The example is very illustrative because it is apparent how
<code>jax.grad</code> works modifying functions; look at the code!</p>
<pre class="python"><code>from jax import grad, vmap, jit

@jax.jit
def tanh(x):
    return (1.0 - jnp.exp(-x))  / (1.0 + jnp.exp(-x))

x = jnp.linspace(-7, 7, 200)
fig, ax = plt.subplots(1, 2, sharey=True, figsize=(12.5, 4.5))
ax[0].plot(x, tanh(x), linestyle=&#39;-&#39;, color=&#39;black&#39;)
ax[0].axis(&#39;off&#39;)
ax[1].plot(x, tanh(x),
           x, vmap(grad(tanh))(x),                               # 1st derivative
           x, vmap(grad(grad(tanh)))(x),                         # 2nd derivative
           x, vmap(grad(grad(grad(tanh))))(x),                   # 3rd derivative
           x, vmap(grad(grad(grad(grad(tanh)))))(x),             # 4th derivative
           x, vmap(grad(grad(grad(grad(grad(tanh))))))(x),       # 5th derivative
           x, vmap(grad(grad(grad(grad(grad(grad(tanh)))))))(x)) # 6th derivative
plt.suptitle(&#39;tanh and its higher-order derivatives (up to 6th)&#39;)           
fig.text(0.75, .02, &quot;Source: Autograd README&quot;, size=9, style=&#39;italic&#39;)
ax[1].axis(&#39;off&#39;)</code></pre>
<center>
<img src="/img/taylor-post/autograd_tanh_example.png">
</center>
<p>As you can see in the code, <code>grad(tanh)</code> gives you a
function to compute the first derivative of <code>tanh</code>. Therefore, the transformation
of <code>jax.grad</code> in math notation is the following.</p>
<p><span class="math display">\[(\nabla f)(x)_i = \frac{\partial f}{\partial x_i}(x)\]</span></p>
<p>Another interesting point is that <code>jax.grad</code> allows you to compose functions in a
series of transformations, such as the nested grad application to compute the
higher-order derivatives of <code>tanh</code>.</p>
<p>Why is the purpose of <code>jax.vmap</code>? If we want that
the function that <code>jax.grad</code> returns behave like this:</p>
<pre class="python"><code>tanh(jnp.array([1.0, 2.0,3.0]))
&gt; DeviceArray([0.46211717, 0.7615941 , 0.90514827], dtype=float32)</code></pre>
<p>We need to vectorize the function. Otherwise, we will have an error.</p>
<pre class="python"><code>grad(tanh)(1.0)
#grad(tanh)(jnp.arange(10)) # throw an error
&gt; DeviceArray(0.39322388, dtype=float32)</code></pre>
<p>Therefore, if we want to evaluate the gradient at multiple values and receive an
array with the results, we can use <code>jax.vmap</code> to transform the function into a
vectorize version as much as grad operates modifying functions.</p>
<pre class="python"><code>vmap(grad(tanh))(jnp.array([1.0, 2.0, 3.0]))
&gt; DeviceArray([0.39322388, 0.20998716, 0.09035333], dtype=float32)</code></pre>
<p>We can code a naive implementation of <code>jax.vmap</code> to
understand what happens behind the scene. Beware that
the original function is far more complex, but this is fair to illustrate the main functionality.</p>
<pre class="python"><code>def my_vmap(x, grad):
  &quot;&quot;&quot;A basic implementation of vmap to vectorize a function&quot;&quot;&quot; 
  FUN = grad
  out = []
  for i in range(x.shape[0]):
    out.append(FUN(x[i]))
  return jnp.array(out)
  
my_vmap(jnp.array([1.0, 2.0, 3.0]), grad(tanh))
&gt; DeviceArray([0.39322388, 0.20998716, 0.09035333], dtype=float32)</code></pre>
<p>You have an idea of how I replicate figure 5.4 of the previous sections that
require computing up to a <em>tenth-order</em> Taylor approximation. Yes, it‚Äôs unnecessary
to hand-code the derivatives. I just used <code>jax.grad</code> ten times over <span class="math inline">\(f\)</span> itself.</p>
<pre class="python"><code>NABLA = FUN
for i in range(NUM):
  # Compute the ith derivative of FUN
  NABLA = jax.grad(NABLA)
  # Do something like computing the ith taylor coefficient
  ...</code></pre>
<p>For instance, let‚Äôs plot <code>tanh</code> and its derivatives, but this time we will differentiate
ten times using the above pattern and avoid the nested code‚Äôs boilerplate.</p>
<pre class="python"><code>NABLA = tanh
for i in range(10):
  plt.plot(x, vmap(NABLA)(x))
  NABLA = grad(NABLA)
plt.axis(&#39;off&#39;)</code></pre>
<center>
<img src="/img/taylor-post/tanh_upto_10diff.png">
</center>
<p>Computing higher-order derivatives can be computationally expensive. Read the paper <a href="https://openreview.net/pdf?id=SkxEF3FNPH">‚ÄúTaylor-Mode Automatic Differentiation for Higher-Order Derivatives in JAX‚Äù</a>
to understand the efficient way to compute higher-order derivatives. More context
about this problem and the paper‚Äôs genesis in this
<a href="https://github.com/google/jax/issues/520">discussion</a>.</p>
<p><strong>How are the derivatives computed?</strong> <code>JAX</code> allow us to perform automatic differentiation and calculates results transforming numerical functions into a directed acyclic graph (DAG):</p>
<ul>
<li>outer lefts nodes represent the input variables</li>
<li>middle nodes represent intermediate variables</li>
<li>the outer right nodes represents the output node (a scalar)</li>
<li>as the name said, there are no cycles in the graph; the data always flows from left to the right, it could have branches, but none edge can point back</li>
</ul>
<p>The differentiation is just an application of the chain rule over DAG.</p>
<br>
<center>
<img src="/img/taylor-post/autoDidf_internediateVar_diagram.png">
</center>
<p>Once we have all the derivatives, we start multiplying but wait, the order matters.
Suppose we begin multiplying the square ‚ÄúF‚Äù, as the diagram above shows you. Using
different orders to compute the gradient can get efficient depending on the problem.</p>
<p><code>jax.make_jaxpr</code> produces the JAX representation of the computation made, and it helps us visualise the diagram described above.</p>
<p>The intermediate variables are equations (<code>jaxpr.eqns</code>) that receive inputs, could be the function‚Äôs input or other intermediate variables, and a set of primitive operations to compute over these to produce outputs.</p>
<p>You can read more about <code>jax.make_jaxpr</code> in the <a href="https://jax.readthedocs.io/en/latest/notebooks/Writing_custom_interpreters_in_Jax.html">documentation</a></p>
<p>For instance, we can inspect how JAX decouples the function <span class="math inline">\(f(x)=x^2 + exp(x)\)</span>
in intermediate variables.</p>
<pre class="python"><code>def f(x):
  return x**2 + jnp.exp(x)</code></pre>
<pre class="python"><code>jax_compu = jax.make_jaxpr(f)(3.0)
jax_compu
&gt; 
 { lambda ; a:f32[]. let
    b:f32[] = integer_pow[y=2] a
    c:f32[] = exp a
    d:f32[] = add b c
  in (d,) }</code></pre>
<p>We can code a function to extract each element of the above <code>jaxpr</code>.</p>
<pre class="python"><code>def describe_jaxpr(FUN):
  &quot;&quot;&quot;Given a function, print each element of its jaxpr&quot;&quot;&quot;
  from inspect import getsource
  print(&#39;Source function definition:&#39;)
  print(getsource(FUN))
  print(&#39;--------------------------------------------------------------&#39;)
  # Evaluate the expression on 0.0 (arbitrary) to get a jaxpr
  expr = jax.make_jaxpr(FUN)(0.0).jaxpr
  print(&#39;The function has the following inputs, represented as &#39; + str(expr.invars))
  print(&#39;the function has the following constants, represented as &#39; + str(expr.constvars))
  # Get the equation that describe each intermediate variable and extract info 
  print(&#39;\nThese are the intermediate variables describe by the equations computed along the DAG: &#39;)
  for i, eq in enumerate(expr.eqns):
    print(&#39;   &#39; + str(i) + &#39;. &#39; + &#39;Obtain &#39; + str(eq[1]) + &#39; applying the primitive &#39; + str(eq.primitive) + &#39; with params &#39; + str(eq.params) + &#39; on input/s &#39; + str(eq[0]))
  print(&#39;\n The output is: &#39; + str(expr.outvars)) </code></pre>
<pre class="python"><code>describe_jaxpr(f)
&gt; 
Source function definition:
def f(x):
 return x**2 + jnp.exp(x)

--------------------------------------------------------------
The function has the following inputs, represented as [a]
the function has the following constants, represented as []

These are the intermediate variables describe by the equations computed along the DAG: 
  0. Obtain [b] applying the primitive integer_pow with params {&#39;y&#39;: 2} on input/s [a]
  1. Obtain [c] applying the primitive exp with params {} on input/s [a]
  2. Obtain [d] applying the primitive add with params {} on input/s [b, c]

The output is: [d]</code></pre>
<p>Similar to the diagram above, we have two intermediate variables used to describe the output in this example.</p>
<ul>
<li>Input <span class="math inline">\(x\)</span> is represented by <span class="math inline">\(a\)</span></li>
<li>The first intermediate variable is <span class="math inline">\(b=a^2\)</span></li>
<li>Then, the second intermediate variable is created also using as input <span class="math inline">\(a\)</span>: <span class="math inline">\(c=exp(a)\)</span></li>
<li>Finally, the output is computed by summing the two intermediate variables: <span class="math inline">\(d=b+c\)</span>.</li>
</ul>
<p>Similarly, we can inspect the gradient function of <span class="math inline">\(f\)</span> given by <code>jax.grad(f)</code>:</p>
<pre class="python"><code>describe_jaxpr(jax.grad(f))
&gt; 
Source function definition:
def f(x):
  return x**2 + jnp.exp(x)

--------------------------------------------------------------
The function has the following inputs, represented as [a]
the function has the following constants, represented as []

These are the intermediate variables describe by the equations computed along the DAG: 
   0. Obtain [b] applying the primitive integer_pow with params {&#39;y&#39;: 2} on input/s [a]
   1. Obtain [c] applying the primitive integer_pow with params {&#39;y&#39;: 1} on input/s [a]
   2. Obtain [d] applying the primitive mul with params {} on input/s [2.0, c]
   3. Obtain [e] applying the primitive exp with params {} on input/s [a]
   4. Obtain [_] applying the primitive add with params {} on input/s [b, e]
   5. Obtain [f] applying the primitive mul with params {} on input/s [1.0, e]
   6. Obtain [g] applying the primitive mul with params {} on input/s [1.0, d]
   7. Obtain [h] applying the primitive add_any with params {} on input/s [f, g]

 The output is: [h]</code></pre>
<p>Notice that the number of intermediate variables increases. For instance, you can
look at the equation described in (2) that is a primitive adding resulting from the
differentiation: <span class="math inline">\(\partial/\partial x (x^2)\rightarrow 2x\)</span>.</p>
<p>Further resources on automatic differentiation and JAX:</p>
<ol style="list-style-type: decimal">
<li><a href="https://www.youtube.com/watch?v=wG_nF1awSS">What‚Äôs automatic differentiation video</a></li>
<li><a href="http://matpalm.com/blog/ymxb_pod_slice">JAX‚Äôs tutorial by Mat Kelcey</a> showing more about
parallel computing using JAX</li>
<li><a href="http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/">Automatic Differentiation, Deep Learning Summer School Montreal 2017 (Matthew Jonhson)</a>; another seminar about the topic <a href="https://www.youtube.com/watch?v=mVf3HJ6gND">JAX seminar</a></li>
</ol>
</div>
<div id="taylor-approximation-with-two-variables" class="section level3">
<h3>3. Taylor Approximation with two variables</h3>
<p>Now we consider the setting when functions are multivariate:</p>
<p><span class="math display">\[f: ‚Ñù^D ‚ü∂ ‚Ñù\]</span>
<span class="math display">\[\quad \quad  \quad  \quad \quad \quad x ‚Ü¶ f(x), \quad x \in ‚Ñù^D\]</span></p>
<p>By definition 5.8 in MML, we have that a Taylor approximation of degree n is defined as:</p>
<p><span class="math display">\[T_n(x)=\sum^{n}_{k=0}\frac{D^k_x f(x_0)}{k!} ùúπ^k\]</span></p>
<p>The vector <span class="math inline">\(ùúπ\)</span> represents a difference between <span class="math inline">\(x\)</span> and <span class="math inline">\(x_0\)</span>; the latter is a pivot-vector in which the approximation is around made.</p>
<p><span class="math inline">\(D^k_x\)</span> and <span class="math inline">\(ùù≥^k\)</span> are tensors or k-dimensionl arrays.</p>
<br>
<blockquote class="twitter-tweet" data-theme="dark">
<p lang="en" dir="ltr">
<a href="https://twitter.com/hardmaru/status/1326054980134973442?s=21">November 10, 2020</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<p><br></p>
<p>If we have that <span class="math inline">\(ùõÖ \in ‚Ñù^4\)</span>, we obtain <span class="math inline">\(ùõÖ^2:=ùõÖ‚®ÇùõÖ=ùõÖùõÖ^T ‚àà ‚Ñù^{4x4}\)</span></p>
<pre class="python"><code>delta=jnp.aarange(4)   # this is [0, 1, 2, 3]
jnp.eisum(&#39;i,j&#39;, delta, delta)
DeviceArray([[0, 0, 0, 0],
             [0, 1, 2, 3],
             [0, 2, 4, 6],
             [0, 3, 6, 9]], dtype=int32)</code></pre>
<p><span class="math inline">\(ùõÖ^3:=ùõÖ‚®ÇùõÖ‚®ÇùõÖ\in ‚Ñù^{4x4x4}\)</span></p>
<pre class="python"><code>jnp.eisum(&#39;i,j,k&#39;, delta, delta, delta)
DeviceArray([[[ 0,  0,  0,  0],
              [ 0,  0,  0,  0],
              [ 0,  0,  0,  0],
              [ 0,  0,  0,  0]],

             [[ 0,  0,  0,  0],
              [ 0,  1,  2,  3],
              [ 0,  2,  4,  6],
              [ 0,  3,  6,  9]],

             [[ 0,  0,  0,  0],
              [ 0,  2,  4,  6],
              [ 0,  4,  8, 12],
              [ 0,  6, 12, 18]],

             [[ 0,  0,  0,  0],
              [ 0,  3,  6,  9],
              [ 0,  6, 12, 18],
              [ 0,  9, 18, 27]]], dtype=int32)</code></pre>
<p>For instance, in the last 4x4x4 array, the last number computed is 64 by
<code>delta[3]*delta[3]*delta[3]</code> (4x4x4). Instead, the most lower-left element of the
third 4x4 array is 48 and you obtained it by <code>delta[2]*delta[3]*delta[3]</code> (3x4x4).</p>
<p>The Einstein Summation implemented in <code>jnp.einsum</code> is a notation that allow you to
represent a lot of array operations using index notation. Look this <a href="https://www.youtube.com/watch?v=pkVwUVEHmfI">video</a> for a detail explanation and the
<a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">documentation</a>.</p>
<p>Let‚Äôs code the example 5.15, deriving at first manually and then use JAX to check
if we reach similar results.</p>
<pre class="python"><code>def g(x, y): 
    &quot;&quot;&quot;Function used in the example 5.15 in MML&quot;&quot;&quot;  
    return x ** 2 + 2 * x * y + y **3</code></pre>
<pre class="python"><code>x = jnp.linspace(-5, 5, 50)
y = jnp.linspace(-5, 5, 40)
X, Y = np.meshgrid(x, y)
Z = g(X, Y)

fig = plt.figure(figsize = (7.2, 4.3))
ax = fig.add_subplot(1, 1, 1, projection=&#39;3d&#39;)
ax.set_title(&quot;$g(x,y)=x^2+2xy+y^3$&quot;)
ax.plot_surface(X, Y, Z, rstride = 3, cstride = 3, cmap = &#39;cividis&#39;,
                antialiased=False, alpha=.6)
ax.set_xlabel(&#39;x&#39;)
ax.set_ylabel(&#39;y&#39;)
ax.set_zlabel(&#39;z&#39;)
ax.zaxis.set_major_locator(
plt.MultipleLocator(60))
plt.subplots_adjust(left=0.0)
ax.view_init(15, 45)</code></pre>
<center>
<img src="/img/taylor-post/taylor_2var_example2.png">
</center>
<p>We will start with the first-order Taylor approximation, which gives us a plane.</p>
<p>We need <span class="math inline">\(\partial g/ \partial x\)</span> and <span class="math inline">\(\partial g/ \partial y\)</span> collect the gradient
into a vector (aka jacobian vector) and multiply by <span class="math inline">\(ùõÖ\)</span>.</p>
<p><span class="math display">\[\partial g/\partial x=2x+2y\]</span>
<span class="math display">\[\partial g/ \partial y=2x+3y^2\]</span></p>
<p>Collect the partials into a vector:</p>
<p><span class="math display">\[D^1_x=\nabla_{x,y}=\big[2x+2y \quad 2x+3y^2\big]\]</span></p>
<p>Following the instruction of the example, we will approximate around <span class="math inline">\((x_0, y_0)=(1,2)\)</span>.</p>
<p>Now we can evaluate all the expressions for completing the equation that describes the plane:</p>
<p><span class="math display">\[T_1(x, y)=f(x_0, y_0) + \frac{D^1_x f(x_0, y_0)}{1!} ùúπ^1\]</span></p>
<pre class="python"><code>def dg_dx(x,y): 
  &quot;&quot;&quot;Derivative of g() w.r.t x hand-coded&quot;&quot;&quot;
  return 2*x + 2*y
  
def dg_dy(x,y): 
  &quot;&quot;&quot;Derivative of g() w.r.t y hand-coded&quot;&quot;&quot;
  return 2*x + 3*y**2</code></pre>
<pre class="python"><code>print(&#39;g(1,2): &#39; + str(g(1,2)))
print(&#39;dg/dx(1,2): &#39; + str(dg_dx(1,2)))
print(&#39;dg/dx(1,2): &#39; + str(dg_dy(1,2)))
&gt; g(1,2): 13
&gt; dg/dx(1,2): 6
&gt; dg/dx(1,2): 14</code></pre>
<p><span class="math display">\[T_1(x, y)=13 + [6 \quad 14] \begin{bmatrix} x-1 \cr y-2 \end{bmatrix}\]</span></p>
<p><br></p>
<p><span class="math display">\[T_1(x, y)=13 + 6(x-1) + 14(y-2)\]</span></p>
<p><br></p>
<p><span class="math display">\[T_1(x, y)=6x+14y-21\]</span></p>
<pre class="python"><code>def g_plane_approx(x, y):
    &quot;&quot;&quot;Equation that describe the tangent plane at g(1,2)&quot;&quot;&quot;
    return 6*x + 14*y - 21 </code></pre>
<center>
<img src="/img/taylor-post/linear_taylor_approx2.png">
</center>
<p><br></p>
<p>Similar to the 1D case, but now the line is a plane. You can notice that is a good
approximation at the very close neighbourhood of the point <span class="math inline">\((x_0, y_0)=(1,2)\)</span>.
However, the plane fails to approximate the curvatures of <span class="math inline">\(g\)</span>.</p>
<p>Now with autodiff‚Ä¶how can we compute the jacobian vector? We can save all the
hand-coded derivatives using the function <code>jax.grad</code>.</p>
<pre class="python"><code>jax.asarray(jax.jacfwd(g, argnums=(0,1))(1.0, 2.0))
&gt; DeviceArray([ 6., 14.], dtype=float32</code></pre>
<p>There is another way to get the jacobian.</p>
<pre class="python"><code>jnp.asarray(jax.jacfwd(g, argnums=(0,1))(1.0, 2.0))</code></pre>
<p><code>jax.jacfwd</code>‚Äôs name stands for jacobian forward and refers to the order that computes
the chain rule. We can use <code>jax.jacrev</code> to obtain the same results but traverse
the graph backwards. There is no concern about which one to use in this example
because the function g is straightforward in complexity. Still, it matters when
many variables are involved, and as a result, we get different shapes of the
jacobian matrix.</p>
<pre class="python"><code>jnp.asarray(jax.jacrev(g, argnums=(0,1))(1., 2.))</code></pre>
<p>The argument <code>argnums</code> specified which with argument differentiate the function.
We give a tuple with the only two arguments of <code>g(x,y)</code>, <em>i.e.¬†I want the full jacobian vector that has the gradient w.r.t. argument 0 (x) and argument 1 (y)</em>.</p>
<p>For example, lets compute the jacobian vector for <span class="math inline">\(x^2+3y+z^2\)</span> and evaluate the
gradient <span class="math inline">\((1.0, 2.0, 2.0)\)</span>.</p>
<pre class="python"><code>jnp.asarray(
            jax.jacfwd(lambda x, y, z: x**2 + 3*y + z**2, 
                                         argnums=(0,1,2))(1.0, 2.0, 2.0)
            )
&gt; DeviceArray([2., 3., 4.], dtype=float32)</code></pre>
<p><em>Note: <code>jax.asarray</code> collect all the derivatives in a single flat array.</em></p>
<p>How can we go further computing the Hessian?</p>
<p>We compute the second-order derivatives of <span class="math inline">\(g\)</span> and collect them into the <span class="math inline">\(H\)</span> matrix.</p>
<p><br></p>
<p><span class="math display">\[H=
\left(\begin{matrix} 
\frac{\partial^2 g}{\partial x^2}=2 &amp; \frac{\partial^2 g}{\partial xy}=2 
\\
\frac{\partial^2 g}{\partial yx}=2 &amp; \frac{\partial^2 g}{\partial y^2}=6y
\end{matrix} \right)\]</span></p>
<p>There are three constants except for the lower-right element of <span class="math inline">\(H\)</span>. We can compute <span class="math inline">\(H\)</span> with two passes of <code>jacfwd</code> and evaluate (1,2) to obtain the Hessian matrix.</p>
<pre class="python"><code>H = jnp.asarray(jax.jacfwd(jax.jacfwd(g, argnums=(0,1)), argnums=(0,1))(1., 2.))
H
&gt; DeviceArray([[ 2.,  2.],
               [ 2., 12.]], dtype=float32)</code></pre>
<p>There is multiple ways to compute the second Taylor‚Äôs polynomial coefficient using the Hessian.</p>
<pre class="python"><code>delta = jnp.array([1., 1.]) -  jnp.array([1.0, 2.0])
jnp.trace(0.5 * H@jnp.einsum(&#39;i,j&#39;, delta, delta))
&gt; DeviceArray(6., dtype=float32)</code></pre>
<pre class="python"><code>0.5 * jnp.einsum(&#39;ij,i,j&#39;, H, delta, delta)
&gt; DeviceArray(6., dtype=float32)</code></pre>
<p>Ok, now we will code a function to compute the Taylor approximation using the above
knowledge.</p>
<pre class="python"><code>def quadratic_taylor_approx(FUN, approx, around_to):
  &quot;&quot;&quot;Compute the quadratic taylor approximation for the set of points &#39;approx&#39; of a given FUN around the. point &#39;around_to&#39;&quot;&quot;&quot;
  delta = approx - around_to
  # Compute the Jacobian and the linear component
  J = jnp.asarray(jax.jacfwd(FUN, argnums=(0,1))(*around_to))
  linear_component = J.dot(delta.T)
  # Compute the Hessian and the qudractic component
  H = jnp.asarray(
                  jax.jacfwd(
                            jax.jacfwd(FUN, argnums=(0,1)), 
                            argnums=(0,1)
                            )(*around_to)
                  )
  quadratic_component = 0.5 * jnp.einsum(&#39;ij, ij-&gt;i&#39;, 
                                         jnp.einsum(&#39;ij,jk-&gt;ik&#39;, delta,H), 
                                         delta)
  return FUN(*around_to) + linear_component + quadratic_component</code></pre>
<pre class="python"><code>quadratic_taylor_approx(g, jnp.array([[1.0, 1.0], [1.0, 2.0], [3.0, 4.0]]), jnp.array([1.0, 2.0]))
&gt; DeviceArray([ 5., 13., 89.], dtype=float32)</code></pre>
<center>
<img src="/img/taylor-post/quadratic_taylor_approx2.png">
</center>
<p>The quadratic component (aka second order derivatives) gives us a better way to
approximate the curvature of <span class="math inline">\(g\)</span>.</p>
<p>Visually it looks ok, but we can use the closed-form expression for the quadratic
Taylor approximation around the point <span class="math inline">\((1, 2)\)</span> to verify if the function
<code>quadratic_taylor_approx</code> is doing its job.</p>
<p><em>Note: You can work out the closed-form expression from equation 5.180c in MML, and ignore the third-order partial derivatives.</em></p>
<p><span class="math display">\[T_2(x, y)=x^2+6y^2-12y+2xy+8\]</span></p>
<pre class="python"><code>def g_quadratic_approx(x, y):
  &quot;&quot;&quot;Close-form expression for the quadratic taylor approx of g() around (1, 2)&quot;&quot;&quot;
  return x**2 + 6*y**2 - 12*y + 2*x*y + 8</code></pre>
<pre class="python"><code># Some cases to test
print(g_quadratic_approx(1.0, 2.0))
print(g_quadratic_approx(2.0, 3.0))
print(g_quadratic_approx(4.2, 3.7))
print(g_quadratic_approx(2.8, 1.3))
print(g_quadratic_approx(10.2, 21))
print(g_quadratic_approx(-5.1, 2.3))
print(g_quadratic_approx(-3.4, -2.5))
&gt; 13.0
&gt; 42.0
&gt; 94.46000000000001
&gt; 17.659999999999997
&gt; 2934.44
&gt; 14.689999999999998
&gt; 104.06</code></pre>
<pre class="python"><code>quadratic_taylor_approx(g, jnp.array([[1.0, 2.0], 
                                      [2.0, 3.0],
                                      [4.2, 3.7],
                                      [2.8, 1.3],
                                      [10.2, 21],
                                      [-5.1, 2.3],
                                      [-3.4, -2.5]
                                      ]), 
                        around_to=jnp.array([1.0, 2.0]))
&gt; DeviceArray([  13.      ,   42.      ,   94.46    ,   17.659998,
             2934.44    ,   14.690002,  104.05999 ], dtype=float32)</code></pre>
<p>The values are practically the same. There are some cases with
approximation error around the thousandth.</p>
</div>
